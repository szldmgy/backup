%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Recent Advances in Variational Principles and The Steepest Descent Type Methods for Matrix Eigenvalue Problems} 
\bb\name{Zhaojun Bai}
\dpt{Department of Computer Science}
\univ{University of California at Davis}
\city{Davis, USA}
\email{bai@cs.ucdavis.edu}

\ec

The variational principles, such as minimax principle andtrace min principle, are of great importance in theoryand computation of Hermitian eigenvalue problem. In this talk,we begin with recent results on the extension of theseprinciples beyond Hermitian eigenvalue problem. Thenwe focus on the application of newly established principlesin the development of the steepest descent and conjugate gradienttype methods for ill-conditioned generalized Hermitian eigenvalueproblems, and linear response eigenvalue problems arisingfrom the density functional theory (DFT) and time-dependent DFTin computational material science.\\ \textrm{} \\This is a joint work with Yunfeng Cai,  Ren-cang Li,Dario Rocca and Giulia Galli.

\tcont{Recent Advances in Variational Principles and The Steepest Descent Type Methods for Matrix Eigenvalue Problems}{Zhaojun Bai}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Representation and Analysis of Continuous Piecewise Linear Functions in Abs-normal Form} 
\bb\name{Andreas Griewank}
\dpt{institute of mathematics}
\univ{Humboldt University}
\city{Berlin, Germany}
\email{kerger@mathematik.hu-berlin.de}

\ec

It follows from the well known min/max representation given by Scholtesin his recent Springer book, that all piecewise linear continuousfunctions \(y = F(x) : \R^n \to \R^m\)  can be written in a so-called abs-normalForm.This means in particular, that all nonsmoothness is encapsulated in s absolutevalue functions that are applied to intermediate switching variables \(z_i\)for \(i=1 \hdots s\). The relation between the vectors \(x, z\)$, and $\(y\) is described byfour matrices \(Y, L, J\), and \(Z\), s.t.\[\left(\begin{matrix} z \\ \textrm{} \\ y  \end{matrix}\right) = \left(\begin{matrix} b \\ \textrm{} \\ c \end{matrix}\right)  + \left(\begin{matrix} Z & L \\ \textrm{} \\ J & Y \end{matrix}\right)  \left(\begin{matrix} x \\ \textrm{} \\ |z| \end{matrix}\right)\]which can be generated by ADOL-C or otherAutomatic Differentation Tools. Hence L is a  lower triangular matrix, and therefor \(z_i\) can be computed successive from previous results.We show that in the square case n=m the system of equations F(x) = 0 canbe rewritten in terms of the variable vector z as a linearcomplementarity problem. The transformation itself and the properties ofthe LCP depend on the Schur complement \(S = L - Z J^{-1} Y\). We discussassociated linear algebra computations and highlight various theoreticaland numerical effects via examples.

\tcont{Representation and Analysis of Continuous Piecewise Linear Functions in Abs-normal Form}{Andreas Griewank}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A derivative-free approach to constrained global optimization based on non-differentiable exact penalty functions} 
\bb\name{Gianni Di Pillo}
\dpt{Faculty of Engineering}
\univ{University of Roma}
\city{Rome, Italy}
\email{dipillo@dis.uniroma1.it}

\ec

In the field of global optimization many efforts have been devoted to globally solving bound constrained optimizationproblems $\{\min f(x), \ l\le x \le u,\ x, l, u \in \Re^n$, $f:\Re^n \rightarrow \Re\}$ without using the derivativesof $f$. In this talk we show how derivative-free bound constrained global optimization methods can be used for globallysolving optimization problems where also general constraints $\{g(x)\le 0, g:\Re^n\rightarrow\Re^p\}$ are present,without using neither the derivatives of $f$ nor the derivatives of $g$. This is of great practical importance in manyreal-world problems where only problem functions values are available.  To our aim we make use of a non-differentiableexact penalty function $P_q(x;\varepsilon)$. We exploit the property that, under weak assumptions, there exists athreshold value $\bar \varepsilon>0$ of the penalty parameter $\varepsilon$ such that, for any $\varepsilon \in (0,\bar \varepsilon]$, any unconstrained global minimizer of $P_q$ is a global solution of the related constrained problemand conversely. On these bases, we describe an algorithm that combines a derivative-free bound constrained globalminimization technique for minimizing $P_q$ for given values of $\varepsilon$ and a derivative-free automatic rule forupdating $\varepsilon$ that acts only a finite number of times. We prove that the algorithm produces a sequence$\{x_k\}$ such that any limit point of the sequence is a global solution of the general constrained problem. In thealgorithm any efficient derivative-free bound constrained global minimization technique can be used. In particular, weadopt an improved version of the DIRECT algorithm. In addition, to improve the performance, the approach is enriched byresorting to local searches, in a multistart framework, based on the proof that for every global minimum point thereexists a neighborhood of attraction for the local search. Some numerical experimentation confirms the effectiveness ofthe approach.\\ \textrm{} \\Joint work with Stefano Lucidi, Francesco Rinaldi.

\tcont{A derivative-free approach to constrained global optimization based on non-differentiable exact penalty functions}{Gianni Di Pillo}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Theory of Tensors (Hypermatrices)} 
\bb\name{Liqun Qi}
\dpt{Department of Applied Mathematics}
\univ{The Hong Kong Polytechnic University}
\city{Hongkong, China}
\email{maqilq@polyu.edu.hk}

\ec

A matrix is an $m \times n$ array of real or complex numbers. The study of matrices haslong been a fundamental tool in mathematical disciplines and many application fields.A tensor (hypermatrix) is an $n_1 \times ... \times n_m$ array of real or complex numbers. In therecent years, several important concepts of matrices, such as eigenvalues, eigenvectors, tensors (hypermatrices) and studied extensively. They have been applied or linkedto some related areas such as tensor approximation, tensor decomposition, higherorder tensor magnetic resonance imaging and spectral hypergraph theory, blind sourceseparation, tensor principal component analysis, etc. It is now a time to make tensor(hypermatrix) theory a mathematical discipline, a useful tool for the above mentionedareas, and a base of tensor theory. Here, the last term tensor" is in the sense ofgeometry or physics.\\ \textrm{} \\Some important concepts of matrices, such as the inverse, can be extended toeven-order tensors (hypermatrices), when the tensors (hypermatrices) are treated asmatrices. On the other hand, some concepts of matrices, can have more than onenatural extensions in tensors (hypermatrices), such as eigenvalues can be extended toeigenvalues or E-eigenvalues, irreducible matrices can be extended to irreducible ten-sors or weakly irreducible tensors, symmetric matrices can be extended to symmetrictensors or strongly symmetric tensors, etc. Thus, tensor (hypermatrix) theory is richto be studied and explored.\\ \textrm{} \\In this talk, I intend to review these issues and the scope of the theory of tensors(hypermatrices).

\tcont{Theory of Tensors (Hypermatrices)}{Liqun Qi}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Numerical Methods for Quadratic Eigenvalue Problems} 
\bb\name{Yangfeng Su}
\dpt{Department of Management Science}
\univ{Fudan University}
\city{Shanghai, China}
\email{yfsu@fudan.edu.cn}

\ec

Quadratic eigenvalue problems (QEPs) appear in almost all vibration analysis of systems, such as buildings, circuits, acoustic structures, and so on. Conventional numerical method for QEPs is to linearize a QEP as a doublly-sized generalized eigenvalue problem (GEP), then call a linear eigen-solver to solve the GEP, e.g. the QZ algorithm for dense GEP, the IRA (implicitly restarted Arnoldi method) for sparseGEP. This method may encounter two difficulties. The first one is that although the linear eigen-solver is good, the eigen-solutions for the original QEP may be bad, in sense of condition number and backward error. The second is that the linearized GEP has its own structure, and the structure is not explorered in a general linear eigen-solver.\\ \textrm{} \\In this talk, we will review recent advances of numerical methods for QEPs.

\tcont{Numerical Methods for Quadratic Eigenvalue Problems}{Yangfeng Su}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Optimization with Semi-Continuous Variables} 
\bb\name{Xiaoling Sun}
\dpt{Department of Management Science}
\univ{Fudan University}
\city{Shanghai, China}
\email{xls@fudan.edu.cn}

\ec

In many real-world applications of optimization models, the decision variables often have to take certain minimum positive values if they are nonzero, due to managerial and technological considerations. Such variables are called semi-continuous variables.  For instance, in the production planning problems, the semi-continuous variables can be used to describe the state of a production process that is either turned off (inactive),hence nothing is produced, or turned on (active) such that the amount of the production has to  lie in certain interval. Optimization problems with semi-continuous variables  can be modeled as mixed-integer programs with specialstructures and are in general NP-hard.\\ \textrm{} \\In this talk, we discuss some recent developments for this class of challenging optimization problems. Our focuses are on efficient MIQP reformulations using SDP and SOCP techniques. In particular, we discuss the relations between perspective reformations and Lagrangian decomposition of the problems. A new lift-and-convexification approach will be also presented. We also report some computational results for different convex reformulations for test problems from portfolio selection, subset selection and compressed sensing.

\tcont{Optimization with Semi-Continuous Variables}{Xiaoling Sun}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A structural geometrical analysis of ill-conditioned semidefinite programs} 
\bb\name{Takashi Tsuchiya}
\univ{National Graduate Research Institute for Policy Studies}
\city{Tokyo, Japan}
\email{tsuchiya@grips.ac.jp}

\ec

While linear programming and semidefinite programing have many nice propertiesin common, they can be quite different in the absence of interior-feasibility.  In particular, the existence of weak infeasibility and finite duality gap is oneof the major difficulties in semidefinite programming.In this talk, we develop a geometrical analysis of ill-conditioned semidefinite programs to shed new light on their common structures.\\ \textrm{} \\This is a joint work with Bruno F. Louren\c{c}o of Tokyo Institute of Technology and Masakazu Muramatsu of the University of Electro-Communications.

\tcont{A structural geometrical analysis of ill-conditioned semidefinite programs}{Takashi Tsuchiya}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Algorithms for Eigenvalue Optimization} 
\bb\name{Zaiwen Wen}
\dpt{Deparment of Mathematics}
\univ{Shanghai Jiaotong University}
\city{Shanghai, China}
\email{zw2109@sjtu.edu.cn}

\ec

Eigenvalue computation has been a fundamental algorithmic component for solving semidefinite programming, low-rank matrix optimization, sparse principal component analysis, sparse inverse covariance matrix estimation, the total energy minimization in electronic structure calculation, and many other data-intensive applications in science andengineering. This talk will present a few recent advance on both linear and nonlinear eigenvalue optimization.

\tcont{Algorithms for Eigenvalue Optimization}{Zaiwen Wen}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Nonlinear High-Dimensional Optimization Problems in Reverse Engineering of Biological Systems} 
\bb\name{Hulin Wu}
\dpt{Department of Biostatistics and Computational Biology}
\univ{University of Rochester}
\city{NewYork, USA}
\email{hulin\_wu@urmc.rochester.edu}

\ec

The cutting-edge and innovative biomedical technologies now are available to produce huge amount of high-throughput and high-resolution complex data to gain insight of a biological system. Recently many new quantitative and computational sciences such as bioinformatics, systems biology and Big Data science have evolved from various disciplines to become major tools to deal with the complex data in biomedical research. A great challenge is to integrate the multi-scale and multi-level high-dimensional data to understand the biological systems and their interactions in a quantitative and dynamic way. In order to quantify a biological system, it is necessary to reverse engineer a mathematical model such as differential equation models based on the high-dimensional and complex experimental data, which usually requires to optimize a nonlinear high-dimensional objective function derived from statistical concepts and methods. This is very challenging from both computational and theoretical perspectives. Computationally it is not an easy task to optimize a nonlinear objective function of a high-dimensional matrix (model parameters) with complex constraints. Theoretically it is not easy to establish the algorithm convergence results since it requires to consider numerical error (deterministic), model approximation error (deterministic), and data noise (random). I will use gene regulatory network modeling as an example to propose these complex optimization problems and issues. Some initial ideas will be proposed to potentially address these problems.

\tcont{Nonlinear High-Dimensional Optimization Problems in Reverse Engineering of Biological Systems}{Hulin Wu}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Numerical Algorithms for Energy Minimization in Electronic Structure Calculation} 
\bb\name{Chao Yang}
\dpt{Computational Research Division }
\univ{Lawrence Berkeley National Laboratory}
\city{Berkeley, USA}
\email{cyang@lbl.gov }

\ec

In Kohn-Sham density functional theory based electronic structure calculation, the ground-state energy of a multi-electron system can be obtained by minimizing the Kohn-Sham total energy functional over a manifold of orthonormal single-particle wavefunctions. One may solve the constrained minimization problem directly, or seek a solution that satisfies the first order necessary condition, which defines a set of nonlinear eigenvalue problems and a fixed point mapthat takes the ground-state electron density to itself. For metallic systems at a non-zero temperature, the Mermin free energy functional is a more suitable objective function to minimize.This functional contains an extra entropy term.  As a result, its first order necessary condition contains an additional equation that defines the chemical potential and occupation numbers for all eigenstates. For this type of problem, the widely used self-consistent field (SCF) iteration seeks the solution to two sets of equations in an alternating fashion. However, it is not clear yet whether an alternating direction method is effective when we try to minimize the Mermin free energy directly. We will examine this problem in detail.

\tcont{Numerical Algorithms for Energy Minimization in Electronic Structure Calculation}{Chao Yang}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Distributed Sparse Optimization} 
\bb\name{Wotao Yin}
\dpt{Department of Computational and Applied Mathematics (CAAM)}
\univ{Rice University}
\city{Los Angels, USA}
\email{wotao.yin@rice.edu}

\ec

Sparse optimization has found interestingapplications in many data-processing areas such ascompressed sensing, machine learning, signalprocessing, medical imaging, finance, etc. Afterreviewing compressed sensing and sparseoptimization, this talk then introduces novelalgorithms tailored for very large scale sparseoptimization problems with very big data. Besides thetypical complexity analysis, we analyze the overheaddue to parallel and distributed computing. Numericalresults are presented to demonstrate the scalability ofthe parallel codes for handling problems withhundreds of gigabytes of data under 2 minutes on theAmazon EC2 cloud computer.\\ \textrm{} \\The work is joint with Zhimin Peng and Ming Yan.

\tcont{Distributed Sparse Optimization}{Wotao Yin}

\np




\part{Contributed Talks}


%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Reducing The Number of Updates for The Limited Memory Quasi-Newton Methods} 
\bb\name{Mehiddin Al-Baali}
\dpt{Department of Mathematics \& Statistics }
\univ{Sultan Qaboos University}
\city{Muscat, Sultanate of Oman}
\email{albaali@squ.edu.om}

\ec

The limited-memory L-BFGS method of Nocedal for large-scale unconstrained optimization will be considered. On each iteration of this method a fixed number, say m, of updates is usually employed. Since the number of function and gradient evaluations required to solve an optimization problem is usually decreased, while the cost of updates is increased, as m increased, the choice of m plays an important role in practice. The possibility of defining m sufficiently large and employing a small number of updates in certain cases will be proposed on the basis of certain simple measures for Hessian approximations. We will focus on our recent damped technique, in modified quasi-Newton methods for unconstrained optimization, which extends that of Powell (1978) in the damped BFGS method for constrained optimization that uses Lagrange functions. Some numerical results will be described to show, in particular, that the proposed measures improve the performance of the L-BFGS method substantially when applied to certain unconstrained optimization problems. Since the damped technique enforces safely the positive definiteness property of any quasi-Newton update, other results will be presented.\\ \textrm{} \\Keywords: Large-Scale Optimization, L-BFGS Method, Damped-Technique, Line Search Framework.

\tcont{Reducing The Number of Updates for The Limited Memory Quasi-Newton Methods}{Mehiddin Al-Baali}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Multi-stage Convex Relaxation for Rank Regularized Minimization Problem} 
\bb\name{Shujun Ben}
\dpt{School of Science}
\univ{Hebei University of Technology}
\city{Tianjin, China}
\email{beamilan@163.com}

\ec

In this talk, we introduce a multi-stage convex relaxation approach to the rank regularized problem with a certain ball constraint. Specifically, we first reformulate this nonconvex and nonsmooth problem as an equivalent continuous augmented optimization problem with a semi-bilinear equality constraint by a variational characterization of rank function, and then show that the penalty problem yielded by adding the semi-bilinear constraint to the objective is exact in the sense that it has the same global optimal solution set as the equivalent continuous augmented optimization problem. By solving the penalty problem in an alternating way, we propose a framework to design the multi-stage convex relaxation approach for the rank regularized problem. This class of approaches consist of solving a sequence of semi-nuclear norm convex regularized problems, and particularly includes the adaptive semi-nuclear norm method with a truncated technique £¨see Miao, Pan and Sun, 2013£©. We apply the multi-stage convex relaxation approach to the rank regularized least square problem, and establish the error bound of the optimal solution of the $k$-th stage to the global optimal solution under some condition weaker than the RIP, which reveals that the proposed multi-stage convex relaxation approach is superior to the nuclear norm convex relaxation. Finally, numerical results are reported to illustrate the efficiency of the proposed method. \\ \textrm{} \\Keywords: rank regularized problem; exact penalty; multi-stage convex relaxation; rank regularized least square.

\tcont{Multi-stage Convex Relaxation for Rank Regularized Minimization Problem}{Shujun Ben}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Complexity Analysis of Interior Point Algorithms for Non-Lipschitz and Nonconvex Minimization} 
\bb\name{Wei Bian}
\dpt{School of Science}
\univ{Harbin Institute of Technology}
\city{Harbin, China}
\email{bianweilvse520@163.com}

\ec

We propose a first order interior point algorithm for a class of non-Lipschitz and nonconvex minimization problemswith box constraints, which arise from applications in variable selection and regularized optimization. The objectivefunctions of these problems are continuously differentiable typically at interior points of the feasible set. Ourfirst order algorithm is easy to implement and the objective function value is reduced monotonically alongthe iteration points. We show that the worst-case iteration complexity for finding an $\epsilon$ scaled firstorder stationary point is $O(\epsilon^{-2})$. Furthermore, we develop a second order interior point algorithmusing the Hessian matrix, and solve a quadratic program with a ball constraint at each iteration. Although thesecond order interior point algorithm costs more computational time than that of the first order algorithm ineach iteration, its worst-case iteration complexity for finding an $\epsilon$ scaled second order stationarypoint is reduced to $O(\epsilon^{-\frac{3}{2}})$. Note that an $\epsilon$ scaled second order stationary pointmust also be an $\epsilon$ scaled first order stationary point.\\ \textrm{} \\This is a join work with Xiaojun Chen and Yinyu Ye.

\tcont{Complexity Analysis of Interior Point Algorithms for Non-Lipschitz and Nonconvex Minimization}{Wei Bian}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Computing k Largest Eigenvalues of Supersymmetric Tensors} 
\bb\name{Chunfeng Cui}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{cuichf@lsec.cc.ac.cn}

\ec

A tensor is a multidimensional array. Computing the largest eigenvalue of super-symmetric tensors is equivalent to solving a constrained homogenous polynomial optimization problem. We put forward a new model for computing the k largest eigenvalues based on the exact Jacobian relaxation.¡¡¡¡Furthermore, it is well-known in numerical algebra that the k-largest eigenvectors and the best rank-k approximation of symmetric matrices are equivalent, yet it is not true for supersymmetric tensors. We will analysis different formulations in the tensor case. Some preliminary numerical results will be presented.\\ \textrm{} \\This work is joint with Prof. Yu-Hong Dai and Prof. Jiawang Nie.

\tcont{Computing k Largest Eigenvalues of Supersymmetric Tensors}{Chunfeng Cui}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A Distributed Quasi-Newton Method for Data Fitting Problem} 
\bb\name{Qian Dong}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{dongqian@lsec.cc.ac.cn}

\ec

Many structured data-fitting applications require the solution of an optimization problem involving a sum over a potentially large number of measurements. We study a Quasi-Newton method that is distributed among the processors. The method involves every processor minimizing its own object function by a Quasi-Newton step while exchanging information locally with other processors in the network over a time-varying topology. Preliminary theoretical analysis and numerical experiments show that this method is promising.

\tcont{A Distributed Quasi-Newton Method for Data Fitting Problem}{Qian Dong}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A Derivative-Free Trust-Region Algorithm for Composite Nonsmooth Optimization} 
\bb\name{Geovani Nunes Grapiglia}
\dpt{Graduate Program in Mathematics and Applied Mathematics}
\univ{Federal University of Paran\'a}
\city{Curitiba,Paran\'a, Brazil}
\email{geovani\_mat@hotmail.com}

\ec

A derivative-free trust-region algorithm is proposed for minimizing the nonsmooth composite function $F(x)=h(f(x))$, where $f$ is smooth and $h$ is convex. This formulation includes pro\-blems of finding feasible points of nonlinear systems of inequalities (where $h(f)\equiv\|f^{+}\|_{p}$, with $f_{i}^{+}=\max\left\{f_{i},0\right\}$ and $1\leq p \leq+\infty$), finite minimax pro\-blems (where $h(f)\equiv\max_{1\leq i\leq m}f_{i}$), and best $L_{1}$, $L_{2}$ and $L_{\infty}$ approximation pro\-blems (where $h(f)\equiv\|f\|_{p}$, $p=1,2,\infty$). The algorithm combine ideas from Powell (1983), Yuan (1985) and Conn, Scheinberg and Vicente (2009). Under some conditions, global convergence results are given. Preliminary numerical tests indicate that the algorithm is promising.

\tcont{A Derivative-Free Trust-Region Algorithm for Composite Nonsmooth Optimization}{Geovani Nunes Grapiglia}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A Quadratic Iterative Method to Compute Eigenvectors} 
\bb\name{Salvador Jimenez}
\dpt{Deparment of Mathematics}
\univ{Universidad Politecnica de Madrid}
\city{Madrid, Spain}
\email{s.jimenez@upm.es}

\ec

We combine two iterative methods, a first one with linear convergence as a primer and a second one with quadratic convergence, and obtain an effectively quadratic iterative method that converges towards an eigenvector of a square matrix that corresponds to the eigenvalue with greater real part. The method can also be adjusted to converge towards eigenvectors of other eigenvalues.\\ \textrm{} \\This is a joint work with L. Vazquez.

\tcont{A Quadratic Iterative Method to Compute Eigenvectors}{Salvador Jimenez}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Dual Purpose Subspace Tracking on Noncompact Stiefel Manifold} 
\bb\name{Lijun Liu}
\dpt{School of Science}
\univ{Dalian Nationalities Universtiy}
\city{Dalian, China}
\email{Liulijun@dlnu.edu.cn}

\ec

Fast estimation and tracking of the principal or minor subspace of a sequence of random vectors is a major problem in many applications. Due to the numerical complexity of the task, eigenvalue decomposition (EVD) cannot be directly performed at every time step. This observation motivates research to find a way to recursively compute the subspace basis, which are usually formulated as optimization problems with orthogonality constraints. However, it is generally difficult to solve such optimization problems since the constraints can lead to many local minimizers and, in particular, several of these problems in special forms are NP-hard. Moreover, even generating a sequence of feasible points is not easy since preserving the orthogonality constraints can be numerically expensive.\\ \textrm{} \\ In this talk, we address the problem of subspace tracking for the principal subspace as well as the minor subspace in a dual learning style utilizing the geometric structure of Grassmann manifold. We restate the subspace tracking problem as an optimization of an extended Rayleigh Quotient on the noncompact Stiefel manifold. A dual purpose gradient procedure for the extended Rayleigh Quotient is obtained by introducing a Riemannian metric on the noncompact Stiefel manifold. Compared to the other subspace tracking algorithms, the proposed algorithm has demonstrated an increased stability, a low complexity and good performances.

\tcont{Dual Purpose Subspace Tracking on Noncompact Stiefel Manifold}{Lijun Liu}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A Node-based SDP Relaxation Approach for Sensor Network Localization} 
\bb\name{Tianxiang Liu}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{liutx@lsec.cc.ac.cn}

\ec

In this report we propose a new model to solve the sensor network localization problem. This method is based on the semidefinite programming (SDP) relaxation, which is actually a further relaxation of the node-based SDP, called RNSDP. To reduce problem scale, in the precondition, we use a trick of edge sparsification. After RNSDP, we use a gradient search method to refine the solution. Numerical results show the efficiency of this method for random produced medium-sized localization problems.

\tcont{A Node-based SDP Relaxation Approach for Sensor Network Localization}{Tianxiang Liu}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Symmetric Low-Rank Product Matrix Approximation and Gauss Newton Method} 
\bb\name{Xin Liu}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{liuxin@lsec.cc.ac.cn}

\ec

We consider computing an eigenspace of an n by n symmetric matrixA corresponding to a set of k largest positive eigenvalues.We derive an efficient formula for applying the Gauss-Newtonmethod to this problem, formulated as minimizing the Frobenius normof A-XX¡¯ where X is n by k and XX¡¯ is called a symmetric low-rankproduct (SLRP). Preliminary numerical results are presented to demonstrate the potential of the algorithm in suitable applications.

\tcont{Symmetric Low-Rank Product Matrix Approximation and Gauss Newton Method}{Xin Liu}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Non-Convex $L_q$ Minimization: Complexity Analysis and A Potential Reduction Algorithm} 
\bb\name{Yafeng Liu}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{yafliu@lsec.cc.ac.cn}

\ec

We consider the $L_q$ minimization problem: finding a minimizer of $\|\max\left\{b-Ax,0\right\}\|_q^q+c^Tx$ subject to $l\leq x\leq u$ for given $A\in\mathcal{R}^{m\times n}, b\in\mathcal{R}^m, c,l,u\in\mathcal{R}^n$ and the parameter $q\in(0,1).$ This problem can be regarded as a non-convex approximation of the sparse $L_0$ minimization problem, which finds a large number of applications such as wireless communications, signal processing, discriminant analysis, and machine learning. In this talk, we shall first give some exact recovery results, i.e., under which conditions, the solution of the $L_q$ minimization problem will be the global minimizer of its corresponding sparse $L_0$ minimization problem. \\ \textrm{} \\ We shall also show that, the $L_q$ minimization problem is strongly NP-hard for any given $q\in(0,1),$ including its smoothed version. Finally, we shall extend the interior-point potential reduction algorithm to solve the $L_q$ minimization problem. The potential reduction algorithm is guaranteed to return an $\epsilon$-KKT solution of the $L_q$ minimization problem in polynomial time.\\ \textrm{} \\This is a joint work with Shiqian Ma, Yu-Hong Dai, and Shuzhong Zhang.

\tcont{Non-Convex $L_q$ Minimization: Complexity Analysis and A Potential Reduction Algorithm}{Yafeng Liu}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Multi-stage Convex Relaxation Approach for PSD Structured Low-rank Optimization Problems} 
\bb\name{Shaohua Pan}
\dpt{School of Science}
\univ{South China University of Technology}
\city{Guangzhou, China}
\email{shhpan@scut.edu.cn}

\ec

This paper is concerned with positive semidefinite (PSD) structured low-rank matrix optimization problems. For this class of NP-hard problems, we use a family of spectral functions to reformulate it as a mathematical program with PSD equilibrium constraints (MPSDEC for short), and show that the penalty problem of this MPSDEC, yielded by adding the equilibrium constraints to the objective, is exact in the sense that it has the same global optimal solution set as the MPSDEC problem when the penalty parameter is over a certain threshold. Then, by solving the exact penalty problem of the MPSDEC in an alternating way, we propose a unified framework to design the multi-stage convex relaxation approach for the PSD structured low-rank optimization problem, which consists of solving a sequence of weighted trace-norm minimization problems. This framework particularly includes the reweighted trace-norm minimization method with a truncated technique (see Mohan 2010 and Miao 2013). For the proposed multi-stage convex relaxation approach, we establish the error bound between the optimal solution of the $k$-th stage and the global optimal solution under a weaker condition than the RIP. Numerical results are also reported for several classes low-rank structured matrix recovery, including the low-rank covariance matrix recovery, the low-rank correlation matrix recovery, the low-rank density matrix recovery, and the low-rank Toeplitz matrix recovery, which verify the efficiency of the proposed approach. \\ \textrm{} \\ Keywords: Multi-stage convex relaxation; structure; low-rank matrix recovery; mathematical program with PSD equilibrium constraints.

\tcont{Multi-stage Convex Relaxation Approach for PSD Structured Low-rank Optimization Problems}{Shaohua Pan}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Data Clustering Based On The Total Variation Energy Functional} 
\bb\name{Zhifeng Pang}
\dpt{School of Mathematics and Information Sciences}
\univ{Henan University}
\city{Henan, China}
\email{zhifengpang@163.com}

\ec

The performance of the data clustering highly relies on the proposed model and the numerical algorithms. Following from the extension of the total variation  functional in the spatially continuous setting, in this report we propose some efficient numerical methods to solve it based on the alternating direction method of multipliers(ADMM) and the primal-dual method(PDM). We show the convergence of proposed numerical methods under the framework of variational inequalities. Some numerical examples are arranged for solving the balanced clustering problem and the unbalanced clustering problem to illustrate the efficiency of our proposed methods.

\tcont{Data Clustering Based On The Total Variation Energy Functional}{Zhifeng Pang}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Partial Convolution for Total Variation Problem by Augmented Lagrangian-based Proximal Point Descent Algorithm} 
\bb\name{Yuan Shen}
\dpt{College of Applied Sciences}
\univ{Nanjing University of Finance \& Economics}
\city{Nanjing, China}
\email{ocsiban@126.com}

\ec

In the field of image processing field, recovering an image from its blurry and noisy observation is a classic problem. A new dedicated alternating minimization method (ADM) arises from a new structured total variation (TV) regularization-based optimization problem has been proposed and intensively studied. This algorithm is called ``Fast Total Variation Deconvolution" (FTVD), and its per-iteration computational cost is dominated by only several fast Fourier transforms (FFT) and convolution operations which are cheap to compute, so they are practical methods. However, this algorithm is only applicable to full convolution model, hence they can not handle more difficult problems. In this paper, we propose a partial convolution model as well as a dedicated algorithm which is based on the idea of Ye and Yuan's ADM. Extensive numerical results show that our algorithm can produce result with much better quality while its speed performance is still comparable with several state-of-the-art algorithms.

\tcont{Partial Convolution for Total Variation Problem by Augmented Lagrangian-based Proximal Point Descent Algorithm}{Yuan Shen}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A Buildup-based Error Minimization Method with Application to Protein Structure Determination} 
\bb\name{Zhenli Sheng}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{szl@lsec.cc.ac.cn}

\ec

Geometric build up method is a fast algorithm particularly designed for distance geometry problem with exact or extremely small noise distances. We incorporate it with error minimization procedure to handle large noise given distances, which are the real-world cases. A new error function has been proposed, and a fast algorithm is designed to minimize the error function. Besides, extensive numerical experiments have been finished on a variety number of proteins, from several hundreds to several thousands, which show that it provides very accurate conformations of these proteins quickly, thus prove that it is a powerful algorithm for this problem.

\tcont{A Buildup-based Error Minimization Method with Application to Protein Structure Determination}{Zhenli Sheng}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Sum Rate Maximization Algorithms for MIMO Relay Networks in Wireless Communications} 
\bb\name{Cong Sun}
\dpt{School of Science}
\univ{Beijing University of Posts and Telecommunications}
\city{Beijing, China}
\email{suncong@lsec.cc.ac.cn}

\ec

Sum rate maximization problem is always of great interests in the field of wireless communications. For MIMO relay networks, we propose a new approach to approximate sum rate maximization, and prove it is a lower bound of achievable sum rate. To solve the nonlinear nonconvex optimization problem, we first change the fraction function into a non-fraction function in the objective function, and show that the optimization problems share the same stationary points. By applying the alternating minimization method, we decompose the complex problem into several subproblems that are easier handled with. Moreover, we prove that the proposed models always lead to rank one solutions. From practical demand, we also add orthogonal constraints and solve the corresponding problem.

\tcont{Sum Rate Maximization Algorithms for MIMO Relay Networks in Wireless Communications}{Cong Sun}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Regularized Mathematical Programs with Stochastic Equilibrium Constraints: Estimating Structural Demand Models} 
\bb\name{Hailin Sun}
\dpt{School of Science}
\univ{Nanjing University of Science \& Technology}
\city{Nanjing, China}
\email{mathhlsun@gmail.com}

\ec

The article considers a particular class of optimization problems involving set-valued stochastic equilibrium constraints. A solution procedure is developed by relying on an approximation scheme for the equilibrium constraints, based on regularization, that replaces them by equilibrium constraints involving only single-valued Lipschitz continuous functions. In addition, sampling has the further effect of replacing the `simplified' equilibrium constraints by more manageable ones obtained by implicitly discretizing the (given) probability measure so as to render the problem computationally tractable. Convergence is obtained by relying, in particular, on the graphical convergence of the approximated equilibrium constraints. The problem of estimating the characteristics of a demand model, a widely studied problem in micro-economics, serves both as motivation and illustration of the regularization and sampling procedure.

\tcont{Regularized Mathematical Programs with Stochastic Equilibrium Constraints: Estimating Structural Demand Models}{Hailin Sun}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Feasible Method for Semi-Infinite Programming} 
\bb\name{Shuxiong Wang}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{wsx@lsec.cc.ac.cn}

\ec

Semi-infinite programming refers to optimization problems involving a finite number of decision variables with an infinite many constraints.        We present an novel feasible method to solve this problem by constructing the inner approximate region of the origin problem. The man idea        is to subdivide the parameter set and construct finite many constraints related to the subdivision points which implies the origin infinitely many constraints.        This approximation guarantees feasibility of the original problem: each iterative point is feasible for the origin problem.        Under standard assumptions, we prove that the solution of the approximate problem converges to the solution of the origin problem.        Numerical experiments show the performance of our method.\\ \textrm{} \\        This is a joint work with Yaxiang Yuan.

\tcont{Feasible Method for Semi-Infinite Programming}{Shuxiong Wang}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{SOARS:  Statistical and Optimization Analysis and Response Surfaces for Computationally Expensive Models} 
\bb\name{Yilun Wang}
\dpt{School of Mathematical Sciences}
\univ{University of Electronic Science and Technology of China}
\city{Chengdu, China}
\email{yilun.wang@gmail.com}

\ec

We are developing a new framework of Statistical and Optimization Analysis and Response Surfaces (SOARS, for short) for Computationally Expensive Objective Functions. The objective functions are computationally expensive often because they are either involving large scale complex computational simulations or complicated data processing procedure.  An important application is parameter calibration of large scale complex model, where the objective function is a distance between the measured data and model output. Unlike most of parameter estimation methods, we are not only searching a single optimal solution, but also calculate its probability distribution and perform related sensitivity analysis.  In brief, (global) optimization, sensitivity analysis and uncertainty analysis are the research tasks. One of our innovations is to build the framework ``SOARS" to integrate them together via the adoption of response surface (also called surrogate, emulator,  or metamodel), and make SOARS suitable for relatively high dimensional problems.  In addition, for each of its component (optimization, sensitivity analysis and uncertainty analysis), we have proposed new efficient algorithms, especially for relatively high dimensional cases.

\tcont{SOARS:  Statistical and Optimization Analysis and Response Surfaces for Computationally Expensive Models}{Yilun Wang}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{On the Convergence Order of the Central Path for Second Order Cone Optimization} 
\bb\name{Zhouhong Wang}
\dpt{School of Science}
\univ{Beijing Jiaotong University}
\city{Beijing, China}
\email{wangzhhng@163.com}

\ec

In this talk, we will discuss the possible convergence order of the central path of Second Order Cone Optimization  (SOCO) based upon the optimal partition for SOCO proposed by Bonnans and Ram\'irez (2005). First we will show that the optimal partition for SOCO can be identified along the central path when the barrier parameter $\mu$ is small enough. Then Some examples are presented to illustrate the possible convergence order of the central path of SOCO.\\ \textrm{} \\Key words: Second Order Cone Optimization; Optimal Partition; Convergence Order of Central Path.

\tcont{On the Convergence Order of the Central Path for Second Order Cone Optimization}{Zhouhong Wang}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{On Minimizing the Ratio of Quadratic Functions over an Ellipsoid} 
\bb\name{Yong Xia}
\dpt{School of Mathematics and System Sciences}
\univ{Beihang University}
\city{Beijing, China}
\email{dearyxia@gmail.com}

\ec

In this talk, we study the optimization problem (RQ) of minimizing the ratio of two quadratic functions over a possibly degenerate ellipsoid. The well-definition of problem (RQ) is fully characterized. We show any well-defined (RQ) admits a semi-definite programming reformulation (SDP) without any assumption. Moreover, the minimum of (RQ) is attained if and only if (SDP) has a unique solution. We finally make some extensions.

\tcont{On Minimizing the Ratio of Quadratic Functions over an Ellipsoid}{Yong Xia}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Rank-one and Sparse Nonnegative Matrix Decomposition for Surveillance Video} 
\bb\name{Xianchao Xiu}
\dpt{School of Science}
\univ{Beijing Jiaotong University}
\city{Beijing, China}
\email{11121760@bjtu.edu.cn}

\ec

This paper presents rank-one and sparse nonnegative matrix decomposition model for surveillance video. Based on its convex relaxation, we establish the alternating direction methods of multipliers. We also introduce a statistical approach to solve the original model with the help of its special properties. Numerical experiments are given to illustrate the efficiency of our algorithms.

\tcont{Rank-one and Sparse Nonnegative Matrix Decomposition for Surveillance Video}{Xianchao Xiu}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Covariance Matrix Estimation Using Factor Models  from Incomplete Information} 
\bb\name{Fangfang Xu}
\dpt{Deparment of Mathematics}
\univ{Shanghai Jiao Tong University}
\city{Shanghai, China}
\email{happyxufangfang@126.com}

\ec

Covariance matrix estimation plays an important role in risk management, asset pricing, and portfolio allocation. This task becomes very challenging when the dimensionality is comparable or much larger than the sample size. A widely used approach for reducing dimensionality is a multi-factor model. Although it has been well studied and quite successful in many applications, the quality of the estimated covariance matrix is often degraded due to a nontrivial amount of missing data in the factor matrix for both technical and cost reasons. Since the factor matrix is only approximately low rank or even has full rank, existing matrix completion algorithms are not applicable. In this paper, we consider a new matrix completion model based on the factor model directly and apply the alternating direction method of multiplier for the recovery. Numerical experiments show that our proposed models and algorithms are helpful.

\tcont{Covariance Matrix Estimation Using Factor Models  from Incomplete Information}{Fangfang Xu}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Joint User Grouping and Linear Virtual Beamforming: Complexity, Algorithms and Approximation Bounds} 
\bb\name{Zi Xu}
\dpt{Deparment of Mathematics}
\univ{Shanghai Jiao Tong University}
\city{Shanghai, China}
\email{xuzi@shu.edu.cn}

\ec

In this work, we consider the problem of properly selecting a subset of users to form the virtual multi-antenna system, while at the same time designing their joint transmission strategies. In the aim of designing practical algorithms with provable theoretical performance, we focus on a class of simple yet important scenarios in which either multiple transmitters cooperatively transmit to a receiver, or a single transmitter transmits to the receiver with the help of a set of cooperative relays. We formulate the joint problems in different settings as cardinality constrained programs that contain both discrete and continuous variables. We then leverage the technique of semi-definite relaxation to obtain approximated solutions for them. The effectiveness of the proposed algorithms is evaluated via both theoretical analysis as well as extensive numerical experiments. We expect that our approach can be applied to solve cross-layer resource allocation problems in many other wireless communication systems as well.

\tcont{Joint User Grouping and Linear Virtual Beamforming: Complexity, Algorithms and Approximation Bounds}{Zi Xu}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Regularizations for Stochastic Linear Variational Inequalities} 
\bb\name{Yanfang Zhang}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{yfzhang@lsec.cc.ac.cn}

\ec

This paper applies the Moreau-Yosida regularization to a convex expected residual minimization formulation for a class of stochastic linear variational inequalities. To have the convexity of the corresponding sample average approximation problem , we adopt the Tikhonov regularization. We show that any cluster point of minimizers of the Tikhonov regularization for the sample average approximation problem is a minimizer of the expected residual minimization formulation with probability one as the sample size goes to infinity and the Tikhonov regularization parameter goes to zero. Moreover, we prove that the minimizer is the least l2-norm solution of the expected residual minimization formulation. We also prove the semi-smoothness of the gradient of the Moreau-Yosida and Tikhonov regularizations for the sample average approximation problem.

\tcont{Regularizations for Stochastic Linear Variational Inequalities}{Yanfang Zhang}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A Subspace Decomposition Framework for Nonlinear Optimization} 
\bb\name{Zaikun  Zhang}
\dpt{Deparment of Mathematics}
\univ{University of Coimbra}
\city{Coimbra, Portugal}
\email{zhang@mat.uc.pt}

\ec

We discuss a general subspace decomposition framework for optimization(for the moment without constraints). Two versions of the framework arepresented, namely a Levenberg-Marquardt version and a trust-region one.We establish global (asymptotic) convergence and derive global rates forboth of them. We also discuss how to exploit the framework to designparallel and multilevel derivative-free algorithms for large-scaleproblems.\\ \textrm{} \\This is a joint work with S. Gratton (ENSEEIHT-INT and CERFACS, France)and L. N. Vicente (University of Coimbra, Portugal).

\tcont{A Subspace Decomposition Framework for Nonlinear Optimization}{Zaikun  Zhang}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{Accelerated Modulus-based Matrix Splitting Iteration Methods for Linear Complementarity Problem} 
\bb\name{Ning Zheng}
\dpt{Department of Mathematics}
\univ{Tongji University}
\city{Shanghai, China}
\email{6zhengning@tongji.edu.cn}

\ec

For the large sparse linear complementarity problem, a class of accelerated modulusbased matrix splitting iteration methods is established by reformulating it as a general implicit fixed-point equation, which covers the known modulus-based matrix splitting iteration methods. The convergence conditions are presented when the system matrix is either a positive definite matrix or an H+-matrix, and the optimal iteration parameters in accelerated modulus-based AOR method are determined by minimizing the spectral radius of the iteration matrix. Numerical experiments further show that the proposed methods are efficient and accelerate the convergence performance of the modulus-based matrix splitting iteration methods with less iteration steps and CPU time.

\tcont{Accelerated Modulus-based Matrix Splitting Iteration Methods for Linear Complementarity Problem}{Ning Zheng}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{New RIC Bounds vea $l_{q}$-minimization with $0 < q \leq 1$ in Compressed Sensing} 
\bb\name{Shenglong Zhou}
\dpt{School of Science}
\univ{Beijing Jiaotong University}
\city{Beijing, China}
\email{Longnan\_zsl@126.com}

\ec

The restricted isometry constants (RICs) play an important role in exact recovery theory of sparse signals via $l_{q}$($0 < q \leq 1$)relaxations in compressed sensing. Recently, Cai and Zhang have achieved a sharp bound $\delta_{tk}<\sqrt{1-1/t}$ for $t \geq \frac{4}{3}$to guarantee the exact recovery of $k$ sparse signals through the $l_{1}$ minimization. This paper aims to establish new RICs bounds via $l_{q}$($0 < q \leq 1$)relaxation. Based on a key inequality on $l_{q}$ norm, we show that (i) the exact recovery can be succeeded via $l_{1/2}$ and $l_{1}$ minimization if $\delta_{tk}<\sqrt{1-1/t}$for $t \geq 1$, (ii) several sufficient conditions can be derived, such as for any $q\in (0,\frac{1}{2})$, $\delta_{2k} < 0.5547$ when $k\geq 2$, for any $q\in (\frac{1}{2})$,$\delta_{2k}<0.6782$ when $k\geq 1$, (iii) the bound on $\delta_{k}$ is given as well as for any $0 < q \leq 1$, especially for $q = \frac{1}{2},1$, we obtain$\delta_{k} < \frac{1}{3}$ when $k$($\geq 2$) is even or $\delta_{k} < 0.3203$ when is  $k$($\geq 3$) odd.\\ \textrm{} \\Keywords: compressed sensing, bound, restricted isometry constant, $l_{q}$ minimization, exact recovery.

\tcont{New RIC Bounds vea $l_{q}$-minimization with $0 < q \leq 1$ in Compressed Sensing}{Shenglong Zhou}

\np

%----------------------------------------------%
%----------------------------------------------%

\bc
\subj{A Modified Primal-dual Augmented Lagrangian Method for Large Scale Nonlinear Optimization} 
\bb\name{Wenwen Zhou}
\univ{SAS}
\email{Wenwen.Zhou@sas.com}

\ec

The Primal Dual Augmented Lagrangian approach [1] has been proposed for large-scale nonconvex optimization.  This approach has a number of promising features including a natural extension to a matrix-free environment. Our numerical experience indicates that the algorithm can have difficulty when the problem is locally infeasible or badly scaled.  The addition of a modified version of the feasibility restoration phase[2] typically used in filter methods is proposed with the added goal of improving Lagrange multiplier estimates when the problem is feasible.

\tcont{A Modified Primal-dual Augmented Lagrangian Method for Large Scale Nonlinear Optimization}{Wenwen Zhou}

\np

\bb \name{Mehiddin Al-Baali}
\dpt{Department of Mathematics \& Statistics }
\univ{Sultan Qaboos University}
\city{Muscat, Sultanate of Oman}
\email{albaali@squ.edu.om}

\bb \name{Congpei An}
\dpt{Deparment of Mathematics}
\univ{Jinan University}
\city{Guangzhou, China}
\email{andbach@163.com}

\bb \name{Xue Bai}
\dpt{School of Science}
\univ{Hebei University of Technology}
\city{Tianjin, China}
\email{baxia\_2011@163.com}

\bb \name{Zhaojun Bai}
\dpt{Department of Computer Science}
\univ{University of California at Davis}
\city{Davis, USA}
\email{bai@cs.ucdavis.edu}

\bb \name{Shujun Ben}
\dpt{School of Science}
\univ{Hebei University of Technology}
\city{Tianjin, China}
\email{beamilan@163.com}

\bb \name{Yaqian Bi}
\dpt{School of Science}
\univ{Hebei University of Technology}
\city{Tianjin, China}
\email{byq\_optim@163.com}

\bb \name{Wei Bian}
\dpt{School of Science}
\univ{Harbin Institute of Technology}
\city{Harbin, China}
\email{bianweilvse520@163.com}

\bb \name{Caihuan Chen}
\dpt{School of Management and Engineering}
\univ{Nanjing University}
\city{Nanjing, China}
\email{ cchenhuayx@gmail.com}

\bb \name{Xingju Cai}
\dpt{School of Mathematical Sciences}
\univ{Nanjing Normal University}
\city{Nanjing, China}
\email{caixingju@njnu.edu.cn}

\bb \name{Liang Chen}
\dpt{School of Mathematical Sciences}
\univ{Huaibei Normal University}
\city{Huaibei, China}
\email{clmyf2@163.com}

\bb \name{Chunfeng Cui}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{cuichf@lsec.cc.ac.cn}

\bb \name{Yuhong Dai}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{dyh@lsec.cc.ac.cn}

\bb \name{Junliang Dong}
\dpt{College of Applied Sciences}
\univ{Beijing University of Technology}
\city{Beijing, China}
\email{dongjl@bjut.edu.cn}

\bb \name{Qian Dong}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{dongqian@lsec.cc.ac.cn}

\bb \name{Jinyan Fan}
\dpt{Deparment of Mathematics}
\univ{Shanghai Jiao Tong University}
\city{Shanghai, China}
\email{jyfan@sjtu.edu.cn}

\bb \name{Geovani Nunes Grapiglia}
\dpt{Graduate Program in Mathematics and Applied Mathematics}
\univ{Federal University of Paran\'a}
\city{Curitiba,Paran\'a, Brazil}
\email{geovani\_mat@hotmail.com}

\bb \name{Andreas Griewank}
\dpt{institute of mathematics}
\univ{Humboldt University}
\city{Berlin, Germany}
\email{kerger@mathematik.hu-berlin.de}

\bb \name{Ming Gu}
\dpt{Department of Mathematics}
\univ{University of California at Berkeley}
\city{Berkeley, USA}
\email{mgu@math.berkeley.edu}

\bb \name{Deren Han}
\dpt{School of Mathematical Sciences}
\univ{Nanjing Normal University}
\city{Nanjing, China}
\email{handeren@njnu.edu.cn}

\bb \name{Le Han}
\dpt{School of Science}
\univ{South China University of Technology}
\city{Guangzhou, China}
\email{hanle@scut.edu.cn}

\bb \name{Jie Hu}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{orsc@amt.ac.cn}

\bb \name{Jianchao Huang}
\dpt{Deparment of Mathematics}
\univ{Shanghai Jiao Tong University}
\city{Shanghai, China}
\email{huangjccn@gmail.com}

\bb \name{Jianlin Jiang}
\dpt{Deparment of Mathematics}
\univ{Nanjing University of Aeronautics and Astronautics}
\city{Nanjing, China}
\email{jiangjianlin\_nju@yahoo.com}

\bb \name{Yanling Jiang}
\dpt{School of Science}
\univ{Hebei University of Technology}
\city{Tianjin, China}
\email{lanbaihe88@126.com}

\bb \name{Salvador Jimenez}
\dpt{Deparment of Mathematics}
\univ{Universidad Politecnica de Madrid}
\city{Madrid, Spain}
\email{s.jimenez@upm.es}

\bb \name{Huilai Li}
\dpt{Mathematics School}
\univ{Jilin University}
\city{Jilin, China}
\email{lihuilai@jlu.edu.cn}

\bb \name{Qingchun Li}
\dpt{School of Mathematics and Statistics}
\univ{Beihua University}
\city{Jilin, China}
\email{Liqingchun01@163.com}

\bb \name{Qiong Li}
\dpt{College of Science}
\univ{China Three Gorges University}
\city{Yichang, China}
\email{liqiongmanj@163.com}

\bb \name{Yong Li}
\dpt{Mathematics School}
\univ{Jilin University}
\city{Jilin, China}
\email{liyong@jlu.edu.cn}

\bb \name{Yusheng Li}
\dpt{School of Mathematical Sciences}
\univ{University of Science and Technology of China}
\city{Hefei, China}
\email{lysh@mail.ustc.edu.cn}

\bb \name{Lijun Liu}
\dpt{School of Science}
\univ{Dalian Nationalities Universtiy}
\city{Dalian, China}
\email{Liulijun@dlnu.edu.cn}

\bb \name{Tianxiang Liu}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{liutx@lsec.cc.ac.cn}

\bb \name{Xin Liu}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{liuxin@lsec.cc.ac.cn}

\bb \name{Xinwei Liu}
\dpt{School of Science}
\univ{Hebei University of Technology}
\city{Tianjin, China}
\email{Optim2008@163.com}

\bb \name{Yafeng Liu}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{yafliu@lsec.cc.ac.cn}

\bb \name{Yigui Ou}
\dpt{College of Information Science \& Technology}
\univ{Hainan University}
\city{Haikou, China}
\email{ouyigui@126.com}

\bb \name{Shaohua Pan}
\dpt{School of Science}
\univ{South China University of Technology}
\city{Guangzhou, China}
\email{shhpan@scut.edu.cn}

\bb \name{Shenglin Pan}
\dpt{Mathematics School}
\univ{Jilin University}
\city{Jilin, China}

\bb \name{Zilian Pan}
\univ{Zxtra-d Life Science}
\email{Celine-pan@yahoo.com}

\bb \name{Zhifeng Pang}
\dpt{School of Mathematics and Information Sciences}
\univ{Henan University}
\city{Henan, China}
\email{zhifengpang@163.com}

\bb \name{Gianni Di Pillo}
\dpt{Faculty of Engineering}
\univ{University of Roma}
\city{Rome, Italy}
\email{dipillo@dis.uniroma1.it}

\bb \name{Liqun Qi}
\dpt{Department of Applied Mathematics}
\univ{The Hong Kong Polytechnic University}
\city{Hongkong, China}
\email{maqilq@polyu.edu.hk}

\bb \name{Yuan Shen}
\dpt{College of Applied Sciences}
\univ{Nanjing University of Finance \& Economics}
\city{Nanjing, China}
\email{ocsiban@126.com}

\bb \name{Zhenli Sheng}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{szl@lsec.cc.ac.cn}

\bb \name{Yangfeng Su}
\dpt{Department of Management Science}
\univ{Fudan University}
\city{Shanghai, China}
\email{yfsu@fudan.edu.cn}

\bb \name{Cong Sun}
\dpt{School of Science}
\univ{Beijing University of Posts and Telecommunications}
\city{Beijing, China}
\email{suncong@lsec.cc.ac.cn}

\bb \name{Hailin Sun}
\dpt{School of Science}
\univ{Nanjing University of Science \& Technology}
\city{Nanjing, China}
\email{mathhlsun@gmail.com}

\bb \name{Liming Sun}
\dpt{School of Mathematics and Statistics}
\univ{Nanjing Audit University}
\city{Nanjing, China}
\email{Sunli\_ming@163.com}

\bb \name{Ting Sun}
\dpt{College of Applied Sciences}
\univ{Beijing University of Technology}
\city{Beijing, China}
\email{971834529@qq.com}

\bb \name{Xiaoling Sun}
\dpt{Department of Management Science}
\univ{Fudan University}
\city{Shanghai, China}
\email{xls@fudan.edu.cn}

\bb \name{Takashi Tsuchiya}
\univ{National Graduate Research Institute for Policy Studies}
\city{Tokyo, Japan}
\email{tsuchiya@grips.ac.jp}

\bb \name{Liping Wang}
\dpt{Deparment of Mathematics}
\univ{Nanjing University of Aeronautics and Astronautics}
\city{Nanjing, China}
\email{wlpmath@nuaa.edu.cn}

\bb \name{Shuxiong Wang}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{wsx@lsec.cc.ac.cn}

\bb \name{Yilun Wang}
\dpt{School of Mathematical Sciences}
\univ{University of Electronic Science and Technology of China}
\city{Chengdu, China}
\email{yilun.wang@gmail.com}

\bb \name{Zhouhong Wang}
\dpt{School of Science}
\univ{Beijing Jiaotong University}
\city{Beijing, China}
\email{wangzhhng@163.com}

\bb \name{Longfei Wei}
\dpt{School of Science}
\univ{Hebei University of Technology}
\city{Tianjin, China}
\email{wlfei2008@126.com}

\bb \name{Lu Wei}
\dpt{College of Applied Sciences}
\univ{Beijing University of Technology}
\city{Beijing, China}
\email{Weilu880604@126.com}

\bb \name{Yimin Wei}
\dpt{Department of Management Science}
\univ{Fudan University}
\city{Shanghai, China}
\email{Yimin.wei@gmail.com}

\bb \name{Zaiwen Wen}
\dpt{Deparment of Mathematics}
\univ{Shanghai Jiaotong University}
\city{Shanghai, China}
\email{zw2109@sjtu.edu.cn}

\bb \name{Hulin Wu}
\dpt{Department of Biostatistics and Computational Biology}
\univ{University of Rochester}
\city{NewYork, USA}
\email{hulin\_wu@urmc.rochester.edu}

\bb \name{Jian Wu}
\dpt{College of Mathematics and Computer Science}
\univ{Gannan Normal University}
\city{Ganzhou, China}
\email{davy.wu@qq.com}

\bb \name{Jiping Wu}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{wjp@lsec.cc.ac.cn}

\bb \name{Yong Xia}
\dpt{School of Mathematics and System Sciences}
\univ{Beihang University}
\city{Beijing, China}
\email{dearyxia@gmail.com}

\bb \name{Dongxiu Xie}
\dpt{School of Science}
\univ{Beijing Information Science \& Technology University}
\city{Beijing, China}

\bb \name{Xinchang Xie}
\dpt{School of Mathematical Sciences}
\univ{University of Science and Technology of China}
\city{Hefei, China}
\email{xcxie@mail.ustc.edu.cn}

\bb \name{Naihua Xiu}
\dpt{School of Science}
\univ{Beijing Jiaotong University}
\city{Beijing, China}
\email{nhxiu@bjtu.edu.cn}

\bb \name{Xianchao Xiu}
\dpt{School of Science}
\univ{Beijing Jiaotong University}
\city{Beijing, China}
\email{11121760@bjtu.edu.cn}

\bb \name{Fangfang Xu}
\dpt{Deparment of Mathematics}
\univ{Shanghai Jiao Tong University}
\city{Shanghai, China}
\email{happyxufangfang@126.com}

\bb \name{Liyan Xu}
\dpt{School of Science}
\univ{Harbin Engineering University}
\city{Harbin, China}
\email{xuliyan@hrbeu.edu.cn}

\bb \name{Zi Xu}
\dpt{Deparment of Mathematics}
\univ{Shanghai Jiao Tong University}
\city{Shanghai, China}
\email{xuzi@shu.edu.cn}

\bb \name{Tao Yan}
\dpt{School of Mathematical Sciences}
\univ{University of Chinese Academy of Sciences}
\city{Beijing, China}
\email{tyan@njust.edu.cn}

\bb \name{Chao Yang}
\dpt{Computational Research Division }
\univ{Lawrence Berkeley National Laboratory}
\city{Berkeley, USA}
\email{cyang@lbl.gov }

\bb \name{Jiaojiao Yang}
\dpt{School of Mathematical Sciences}
\univ{University of Science and Technology of China}
\city{Hefei, China}
\email{jiao904@mail.ustc.edu.cn}

\bb \name{Junfeng Yang}
\dpt{Deparment of Mathematics}
\univ{Nanjing University}
\city{Nanjing, China}
\email{jfyang@nju.edu.cn}

\bb \name{Yueting Yang}
\dpt{School of Mathematics and Statistics}
\univ{Beihua University}
\city{Jilin, China}
\email{Yueting\_yang@126.com}

\bb \name{Zhouwang Yang}
\dpt{School of Mathematical Sciences}
\univ{University of Science and Technology of China}
\city{Hefei, China}
\email{yangzw@ustc.edu.cn}

\bb \name{Hongxia Yin}
\dpt{Department of Mathematics and Statistics}
\univ{Minnesota State University}
\city{Mankato, USA}
\email{skye.dauer@mnsu.edu}

\bb \name{Wotao Yin}
\dpt{Department of Computational and Applied Mathematics (CAAM)}
\univ{Rice University}
\city{Los Angels, USA}
\email{wotao.yin@rice.edu}

\bb \name{Gaohang Yu}
\dpt{College of Mathematics and Computer Science}
\univ{Gannan Normal University}
\city{Ganzhou, China}
\email{maghyu@163.com}

\bb \name{Ya-xiang Yuan}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{yyx@lsec.cc.ac.cn}

\bb \name{Haibin Zhang}
\dpt{College of Applied Sciences}
\univ{Beijing University of Technology}
\city{Beijing, China}
\email{zhanghaibin@bjut.edu.cn}

\bb \name{Shaoliang Zhang}
\univ{Nagoya University}
\city{Nagoya, Aichi, Japan}
\email{zhang@na.cse.nagoya-u.ac.jp}

\bb \name{Yanfang Zhang}
\dpt{Academy of Mathematics and Systems Science}
\univ{Chinese Academy of Sciences}
\city{Beijing, China}
\email{yfzhang@lsec.cc.ac.cn}

\bb \name{Zaikun  Zhang}
\dpt{Deparment of Mathematics}
\univ{University of Coimbra}
\city{Coimbra, Portugal}
\email{zhang@mat.uc.pt}

\bb \name{Ning Zheng}
\dpt{Department of Mathematics}
\univ{Tongji University}
\city{Shanghai, China}
\email{6zhengning@tongji.edu.cn}

\bb \name{Shenglong Zhou}
\dpt{School of Science}
\univ{Beijing Jiaotong University}
\city{Beijing, China}
\email{Longnan\_zsl@126.com}

\bb \name{Wenwen Zhou}
\univ{SAS}
\email{Wenwen.Zhou@sas.com}

\bb \name{Yongkui Zou}
\dpt{Mathematics School}
\univ{Jilin University}
\city{Jilin, China}
\email{zouyk@jlu.edu.cn}

